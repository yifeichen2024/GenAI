{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Signature\n",
    "\n",
    "Please implement the `signature()` function in the `submission.py` file. It should return your first name and last name as a string. This is free points. Failure to do so will result in a 0 on the assignment. The function is on line 19.\n",
    "\n",
    "Run the following cell to check your signature is returning your name as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YifeiChen\n"
     ]
    }
   ],
   "source": [
    "print(submission.signature())\n",
    "assert type(submission.signature()) == str, 'Signature should return a string'\n",
    "# this should return your name as a string, not a None type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the attention mechanism with the `Attention` class. \n",
    "\n",
    "You have learned that the attention mechanism is a way to compute the importance of each \"word\" in a sequence. In this part of the assignment, you will implement the attention mechanism in the `Attention` class. \n",
    "\n",
    "This is a toy attention mechanism. The outline of our attention mechanism is as follows:\n",
    "\n",
    "1. You give it a sentence.\n",
    "2. You will compute the word embeddings and positional embeddings for the sentence. This function has already been implemented for you. But when you run your attention mechanism you will need to first add the word embeddings and positional embeddings together first.\n",
    "3. You will cast the embeddings to the dimensionality of the hidden layers.\n",
    "4. Using the hidden embeddings and the weight matrices, you will compute the attention scores and the attention weighted embeddings.\n",
    "5. You will report the attention scores.\n",
    "6. You will report the value embeddings that have been weighted by the attention scores. \n",
    "7. The weighted value embeddings you report should be cast back to the dimensionality of the original word embeddings.\n",
    "\n",
    "The `Attention` class will have the following methods:\n",
    "\n",
    "- `__init__`: This method will initialize the class. It should define the Q, K, V weight matrices, and the maximum number of words in a sequence. Implement this method. \n",
    "- `get_word_embeddings`: This method will return the embeddings for words that you want to compute the attention for. There is no need to implement this method. It has been implemented for you. \n",
    "- `get_positional_embeddings`: This method will return the positional embeddings for words that you want to compute the attention for. Implement this method.\n",
    "- `get_attention`: This method will compute the attention mechanism. Implement this method. \n",
    "\n",
    "Please see the instructions within each function for more details. \n",
    "\n",
    "## Hints \n",
    "\n",
    "- The `softmax` function is imported from `scipy.special`. You may use this in the attention mechanism. \n",
    "- Use the `@` operator to perform matrix multiplication. \n",
    "- Think carefully about the dimensions of the matrices you are multiplying. \n",
    "\n",
    "## Grading\n",
    "\n",
    "You will be graded on the following criteria:\n",
    "\n",
    "- The `get_attention` method is implemented correctly. We will test your implementation on unit tests.\n",
    "\n",
    "Go to the `get_attention` function in the `submission.py` file to implement your code. It is at line 110, but you may want to read the entire class to understand the context. \n",
    "\n",
    "Run the following cell to test your implementation. It should pass if your implementation is bug free. Please do you due diligence to ensure your implementation is correct. One way is to use a small matrix with simple numbers and do the math by hand. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings (5, 16)\n",
      "pos_embeddings (5, 16)\n",
      "attn_score (5, 5)\n",
      "attn_weighted_embeddings (5, 16)\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "def test_attention_p1(sentence: str):\n",
    "    attn = submission.Attention()\n",
    "    embeddings, pos_embeddings = attn.get_embeddings(sentence)\n",
    "    print('embeddings', embeddings.shape)\n",
    "    print('pos_embeddings', pos_embeddings.shape)\n",
    "    attn_score, attn_weighted_embeddings = attn.get_attention(embeddings, pos_embeddings)\n",
    "    print('attn_score', attn_score.shape)\n",
    "    print('attn_weighted_embeddings', attn_weighted_embeddings.shape)\n",
    "    \n",
    "    assert attn_weighted_embeddings.shape == embeddings.shape\n",
    "    print('Test passed')\n",
    "\n",
    "test_attention_p1('this is a test sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch to implement the attention mechanism\n",
    "\n",
    "We want to use PyTorch to so we can use its autograd capabilities and actually train this model to do something. \n",
    "\n",
    "Below is the `AttentionTransformer` class that has parts to be implemented. It represents a transformer with a single head attention mechanism, but without the AddNorm. \n",
    "\n",
    "This transformer will be used as a classifier downstream. Specifically, we will be using the transformer to detect the number of times \"cat\" appears in a sentence. \n",
    "\n",
    "Only implement the methods and sections that are marked with TODO. If you are not familiar with PyTorch, you should refer to the PyTorch documentation to understand the methods that are being used. \n",
    "\n",
    "## Grading\n",
    "\n",
    "You will be graded on the following criteria:\n",
    "\n",
    "- The `AttentionTransformer` class is implemented correctly. We will test your implementation on unit tests.\n",
    "\n",
    "Go to the `AttentionTransformer` class in the `submission.py` file to implement your code. \n",
    "\n",
    "It is at line 218, but you may want to read the entire class to understand the context. \n",
    "\n",
    "After you have implemented the `attention` method, run the following cell to test your implementation. It should pass if your implementation is bug free. Please do you due diligence to ensure your implementation is correct. One way is to use a small matrix with simple numbers and do the math by hand. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5])\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def test_attention_p2():\n",
    "    attn = submission.AttentionTransformer(max_input_length=10, embedding_dim=10, hidden_dim=5)\n",
    "    attn_weighted_embeddings = attn.attention(torch.randn(1, 10, 10))\n",
    "    print(attn_weighted_embeddings.shape)\n",
    "    assert attn_weighted_embeddings.shape == torch.randn(1, 10, 5).shape, 'You have a dimension mismatch'\n",
    "    print('Test passed')\n",
    "    \n",
    "test_attention_p2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training the Attention Transformer\n",
    "\n",
    "A common way to train models and maintain hyperparameters is to use a dataclass. On line 256 of the `submission.py` file, you will see a dataclass called `TrainingParameters`. \n",
    "\n",
    "Fill out the parameters in the dataclass in the `submission.py` file, then run the cell below to train your model. You can observe the training loss and evaluation score.\n",
    "\n",
    "You should be able to get a score of 1.0 on the evaluation score if your implementation is correct and hyperparameters are set appropriately.\n",
    "\n",
    "Grading:\n",
    "\n",
    "- The accuracy of the model on the test set = 1, full score \n",
    "- The accuracy of the model on the test set = 0.7, 50% score\n",
    "- The accuracy of the model on the test set = 0.4, 25% score\n",
    "- The accuracy of the model on the test set < 0.4, no points\n",
    "\n",
    "Please ensure your implementation is bug free in `submission.py`. A quick sanity check is to run the cell below and see if it runs without errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 400/400 [00:00<00:00, 4073.73 examples/s]\n",
      "Map: 100%|██████████| 1600/1600 [00:00<00:00, 4328.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 2.385319764797504\n",
      "Epoch 0 eval accuracy: 0.2575\n",
      "Epoch 10 train loss: 1.5432332020539503\n",
      "Epoch 10 eval accuracy: 1.0\n",
      "Epoch 20 train loss: 1.5431088759348943\n",
      "Epoch 20 eval accuracy: 1.0\n",
      "Epoch 30 train loss: 1.5430772304534912\n",
      "Epoch 30 eval accuracy: 1.0\n",
      "Epoch 40 train loss: 1.5430635488950288\n",
      "Epoch 40 eval accuracy: 1.0\n",
      "Epoch 50 train loss: 1.543056240448585\n",
      "Epoch 50 eval accuracy: 1.0\n",
      "Epoch 60 train loss: 1.5430519672540517\n",
      "Epoch 60 eval accuracy: 1.0\n",
      "Epoch 70 train loss: 1.5430492346103375\n",
      "Epoch 70 eval accuracy: 1.0\n",
      "Epoch 80 train loss: 1.5430473272617047\n",
      "Epoch 80 eval accuracy: 1.0\n",
      "Epoch 90 train loss: 1.5430458784103394\n",
      "Epoch 90 eval accuracy: 1.0\n",
      "1.543044979755695\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "params = submission.TrainingParameters()\n",
    "params.epochs = 100\n",
    "\n",
    "loss_history, eval_score_history, model = submission.train(params, data_path='sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your performance on the following sentence provided in `test`\n",
    "\n",
    "You are free to change the sentence to test your model. The model here is the one you trained in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "no-export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction of number of cats: 7\n",
      "True number of cats: 7\n"
     ]
    }
   ],
   "source": [
    "test = 'catcat is a great pet. tac cat act cactus cat cat is a great pet. tac act cat us cat' \n",
    "\n",
    "tokenizer = params.tokenizer\n",
    "\n",
    "tokens = tokenizer(test, \n",
    "                   return_tensors='pt', \n",
    "                   padding=\"max_length\", \n",
    "                   truncation=True, \n",
    "                   max_length=params.token_max_length).to(params.device)['input_ids']\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.forward(tokens).argmax(dim=-1).item()\n",
    "    print(f'Model prediction of number of cats: {output}')\n",
    "    print('True number of cats:', test.count('cat'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
